{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback\n",
    "## Exercise 1\n",
    "1. no sé que hicieron para cargar los datos, pero es tremendamente lento. Observaciones:\n",
    "    - los preprocessors usan datos de la validación!!\n",
    "    - en general no está limpio. Aplican one hot al final otra vez ... no sé que es esto. Preciso para esto usamos pipelines ... 0.5/1.5\n",
    "2. ok. 0.5/0.5\n",
    "    \n",
    "## Exercise 2\n",
    "1. good.1.5/1.5\n",
    "\n",
    "2. good. 0.5/0.5\n",
    "\n",
    "\n",
    "## Exercise 3\n",
    "1. bien, pero se debería retornar una columna *por cada atributo* y no hacer un resumen de los atributos en una columna. 0.75/1\n",
    "2. esto es confuso. Realmente no entiendo esta función. Aparentemente siempre retorna 1 en el score. El wrapping aparantamente usa todos los datos, lo cual obviamente está prohibido. 0.25/1\n",
    "3. no me queda clara esta interpretación. Al parecer pierden 10% de accuracy en madelon en el test set. Aquí hay un problema. 0.5/1\n",
    "\n",
    "Total: 0.5 + 0.5 + 1.5 +0.5 + 0.75 + 0.25 + 0.5 = 4.5/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU1bFmsiCu3h"
   },
   "source": [
    "# Universidad de La Sabana \n",
    "\n",
    "**Felix Mohr**\n",
    "\n",
    "## Facultad de Ingenería 2022.2\n",
    "Supervised Machine Learning\n",
    "\n",
    "\n",
    "- Gerson Barrera\n",
    "- Santiago Merchán\n",
    "- Rodrigo Carranza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVNSaLW4iPkc"
   },
   "source": [
    "On this sheet, we are working with the datasets mushroom, madelon and amazon-commerce-reviews from openml.org. You can use the API from openml.org to download these datasets (ids 24,1485, and 1457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWp9ym01ykit"
   },
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tPnNvTpuCu3l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg\n",
    "import scipy.stats\n",
    "from scipy.stats import norm as normal\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.discriminant_analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.tree\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.datasets as dfs\n",
    "import sklearn.metrics\n",
    "from numpy import percentile\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from numpy import linspace\n",
    "import random\n",
    "%matplotlib inline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import power_transform \n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jprg7JL6OTNs"
   },
   "outputs": [],
   "source": [
    "#Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "#Model Selection\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sklearn.tree\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nqb4RjdD38V0"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD6495XpyhGK"
   },
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zo29ExLgCu3m"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e02705757ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# List all datasets and their properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Get dataset by ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36mlist_datasets\u001b[0;34m(data_id, offset, size, status, tag, output_format, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openml/utils.py\u001b[0m in \u001b[0;36m_list_all\u001b[0;34m(listing_call, output_format, *args, **filters)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mactive_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenMLServerNoResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36m_list_datasets\u001b[0;34m(data_id, output_format, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mapi_call\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"/data_id/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m__list_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36m__list_datasets\u001b[0;34m(api_call, output_format)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__list_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mxml_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0mdatasets_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmltodict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oml:dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openml/_api_calls.py\u001b[0m in \u001b[0;36m_perform_api_call\u001b[0;34m(call, request_method, data, file_elements)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_url_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__read_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0m__check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openml/_api_calls.py\u001b[0m in \u001b[0;36m__read_url\u001b[0;34m(url, request_method, data, md5_checksum)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_key\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapikey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     return _send_request(\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mrequest_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5_checksum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5_checksum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/openml/_api_calls.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(request_method, url, data, files, md5_checksum)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrequest_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mrequest_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"delete\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    585\u001b[0m         }\n\u001b[1;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    497\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m                 )\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m             )\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         )\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         )\n\u001b[1;32m    452\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m    868\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import openml\n",
    "\n",
    "# List all datasets and their properties\n",
    "openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "\n",
    "# Get dataset by ID\n",
    "mushroom = openml.datasets.get_dataset(24)\n",
    "madelon = openml.datasets.get_dataset(1485)\n",
    "amazon = openml.datasets.get_dataset(1457)\n",
    "\n",
    "\n",
    "# Get the data itself as a dataframe (or otherwise)\n",
    "mushroom_df, y, _, _  = mushroom.get_data(dataset_format=\"dataframe\")\n",
    "madelon_df, y, _, _  = madelon.get_data(dataset_format=\"dataframe\")\n",
    "amazon_df, y, _, _  = amazon.get_data(dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21Ex4hVOgHI0"
   },
   "outputs": [],
   "source": [
    "X_madelon, y_madelon = madelon_df.drop(\"Class\", axis=1), madelon_df[\"Class\"].astype(str)\n",
    "X_amazon, y_amazon = amazon_df.drop(\"Class\", axis=1), amazon_df[\"Class\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0mh8UH0r2ag"
   },
   "source": [
    "#**Exercise 1** (Pre-Processing)\n",
    "\n",
    "1. Write a function get_best_encoding(df, label, learner, scoring), where df is a pandas dataframe and label is the target column. The function should look at combinations of: \n",
    "\n",
    "- dropping or keeping a column for the first value in each category \n",
    "- every technique of imputation (if the dataset has missing data).\n",
    "\n",
    "  For each combination, mount a pipeline with the respective pre-processing steps and learner, and check the score of a cross-validation given the scoring function. Return the pair X, y, where X is the encoding for which the best result was obtained. \n",
    "\n",
    "2. Apply the function to the mushroom dataset once for kNN and once for a decision tree. Look at the class distribution and choose an appropriate scoring function (justify yourchoice!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoH1WWb1zCmT"
   },
   "source": [
    "1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SeN8EmvnaP8"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def one_hot(df, drop, handle_unknown):\n",
    "  \n",
    "  #one_hot = OneHotEncoder(drop=\"first\")\n",
    "  #one_hot_df =  pd.DataFrame( one_hot.fit_transform(X[X.columns]))\n",
    "\n",
    "  encoder = OneHotEncoder(drop=drop, handle_unknown=handle_unknown,  sparse=False).fit(df[df.columns])\n",
    "  test = pd.DataFrame(encoder.transform(df[df.columns]))\n",
    "  test.columns = encoder.get_feature_names(df.columns)\n",
    "\n",
    "  return test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def label_encoder(X, y):\n",
    "  Encoder_X = LabelEncoder() \n",
    "  for col in X.columns:\n",
    "      X[col] = Encoder_X.fit_transform(X[col])\n",
    "\n",
    "  Encoder_y=LabelEncoder()\n",
    "  y = Encoder_y.fit_transform(y)\n",
    "  return X, y\n",
    "\n",
    "def get_column_types(df):\n",
    "  A = df.select_dtypes(include='number').columns\n",
    "  print(\"Numeric attributes: \", A)\n",
    "  B = df.select_dtypes(exclude='number').columns\n",
    "  #B = df.select_dtypes(['category']).columns\n",
    "  print(\"Categorical attributes: \", B)\n",
    "  return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVOwy8xv30Xt"
   },
   "outputs": [],
   "source": [
    "def get_best_encoding(df, label, lerner, scoring):\n",
    "\n",
    "  X_ = df.drop(label, axis=1)\n",
    "  y_ = df[label]\n",
    "  result = []\n",
    "  drops = []\n",
    "  unknows = []\n",
    "\n",
    "  a, b = get_column_types(X_)\n",
    "  matrix_elements = []\n",
    "\n",
    "  drop_list = [None, \"first\", \"if_binary\"]\n",
    "  handle_unknown = [\"error\", \"ignore\"]\n",
    "  imputers_list = ['most_frequent', 'constant']\n",
    "\n",
    "  for i in drop_list:\n",
    "      for j in imputers_list:\n",
    "\n",
    "        # Transformaciones para las variables numéricas\n",
    "        numeric_transformer = Pipeline(\n",
    "                                steps=[\n",
    "                                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                                    ('scaler', StandardScaler())\n",
    "                                ]\n",
    "                              )\n",
    "\n",
    "\n",
    "        # Transformaciones para las variables categóricas\n",
    "        categorical_transformer = Pipeline(\n",
    "                                    steps=[\n",
    "                                        ('imputer', SimpleImputer(strategy=j)),\n",
    "                                        ('onehot', OneHotEncoder(drop=i, handle_unknown='ignore'))\n",
    "                                    ]\n",
    "                                  )\n",
    "        if len(a)>0:\n",
    "          preprocessor = ColumnTransformer(\n",
    "                              transformers=[\n",
    "                                  ('numeric', numeric_transformer, a),\n",
    "                                  ('cat', categorical_transformer, b)\n",
    "                              ],\n",
    "                              remainder='passthrough'\n",
    "                          )\n",
    "          \n",
    "        else:\n",
    "          preprocessor = ColumnTransformer(\n",
    "                              transformers=[\n",
    "                                  ('cat', categorical_transformer, b)\n",
    "                              ],\n",
    "                              remainder='passthrough'\n",
    "                          )      \n",
    "        \n",
    "        X_train_prep = preprocessor.fit_transform(X_)\n",
    "        #X_test_prep  = preprocessor.transform(X_test)\n",
    "\n",
    "        encoded_cat = preprocessor.named_transformers_['cat']['onehot']\\\n",
    "                      .get_feature_names(b)\n",
    "        labels = np.concatenate([a, encoded_cat])\n",
    "        datos_train_prep = preprocessor.transform(X_)\n",
    "        \n",
    "        datos_train_prep = pd.DataFrame(datos_train_prep, columns=labels)\n",
    "        clf = lerner\n",
    "        res = np.mean(sklearn.model_selection.cross_validate(estimator=clf, X=datos_train_prep, y=y_, cv=5, n_jobs=4, scoring = scoring)['test_score'])\n",
    "\n",
    "\n",
    "        result.append(res)\n",
    "        drops.append(i)\n",
    "        unknows.append(j)\n",
    "\n",
    "\n",
    "        matrix_elements.append((res, i, j))\n",
    "\n",
    "\n",
    "  df = pd.DataFrame(data = matrix_elements, columns = ['score', 'imputation', 'drop'])\n",
    "  df.score = round(df.score, 2)\n",
    "  df_matriz = pd.pivot_table(df, values='score', index=['drop'],  columns=['imputation'])\n",
    "\n",
    "  a, b = df_matriz.stack().idxmax()\n",
    "  s = df_matriz.loc[[a], [b]].values\n",
    "  print(\"La mejor configuración posible con este Dataset es con drop:\", a,\"\\n\",  \"Handle unknown:\", b,\"\\n\", \"Score:\", s ) \n",
    "  X_  = one_hot(X_, a, b)\n",
    "  return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT39aqLgJRbl"
   },
   "outputs": [],
   "source": [
    "def get_best_encoding(df, label, lerner, scoring):\n",
    "\n",
    "  drop_list = [None, \"first\", \"if_binary\"]\n",
    "  handle_unknown = [\"error\", \"ignore\"]\n",
    "\n",
    "  a, b = get_column_types(df)\n",
    "\n",
    "  for i in a:\n",
    "    df[i] = df[i].fillna(df[i].mean()) # Imputation for Numerical Data\n",
    "\n",
    "  for i in b:\n",
    "    df[i] = df[i].fillna(df[i].mode()) # Imputation for Categorical Data\n",
    "\n",
    "\n",
    "  X_ = df.drop(label, axis=1)\n",
    "  y_ = df[label]\n",
    "\n",
    "  result = []\n",
    "  drops = []\n",
    "  unknows = []\n",
    "\n",
    "  matrix_elements = []\n",
    "  for i in drop_list:\n",
    "    for j in handle_unknown:\n",
    "\n",
    "      X_  = one_hot(X_, i, j)\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.3)\n",
    "      \n",
    "      clf = lerner.fit(X_train,y_train)\n",
    "      predicted = clf.predict(X_test)\n",
    "      predicted_proba = clf.predict(X_test)\n",
    "      res = np.mean(sklearn.model_selection.cross_validate(estimator=clf, X=X_train, y=y_train, cv=5, n_jobs=4, scoring = scoring)['test_score'])\n",
    "\n",
    "      result.append(res)\n",
    "      drops.append(i)\n",
    "      unknows.append(j)\n",
    "\n",
    "      matrix_elements.append((res, i, j))\n",
    "      \n",
    "      \n",
    "  df = pd.DataFrame(data = matrix_elements, columns = ['score', 'drop_criterio', 'unknown'])\n",
    "  df.score = round(df.score, 2)\n",
    "  df_matriz = pd.pivot_table(df, values='score', index=['drop_criterio'],  columns=['unknown'])\n",
    "\n",
    "  a, b = df_matriz.stack().idxmax()\n",
    "  s = df_matriz.loc[[a], [b]].values\n",
    "  print(\"La mejor configuración posible con este Dataset es con drop:\", a,\"\\n\",  \"Handle unknown:\", b,\"\\n\", \"Score:\", s ) \n",
    "  X_  = one_hot(X_, a, b)\n",
    "  return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbM6oXioU9U4",
    "outputId": "e244a237-2714-42ac-f207-fb2e6ee0f8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric attributes:  Index([], dtype='object')\n",
      "Categorical attributes:  Index(['cap-shape', 'cap-surface', 'cap-color', 'bruises%3F', 'odor',\n",
      "       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
      "       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
      "       'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
      "       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
      "       'ring-type', 'spore-print-color', 'population', 'habitat', 'class'],\n",
      "      dtype='object')\n",
      "La mejor configuración posible con este Dataset es con drop: first \n",
      " Handle unknown: error \n",
      " Score: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "X_mushroom, y_mushroom = get_best_encoding(mushroom_df, \"class\",KNeighborsClassifier(), \"accuracy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuy1tRluZaeS",
    "outputId": "404ab47b-13c6-40fd-9394-09368d75e2be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric attributes:  Index([], dtype='object')\n",
      "Categorical attributes:  Index(['cap-shape', 'cap-surface', 'cap-color', 'bruises%3F', 'odor',\n",
      "       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
      "       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
      "       'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
      "       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
      "       'ring-type', 'spore-print-color', 'population', 'habitat', 'class'],\n",
      "      dtype='object')\n",
      "La mejor configuración posible con este Dataset es con drop: first \n",
      " Handle unknown: error \n",
      " Score: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "X_mushroom, y_mushroom = get_best_encoding(mushroom_df, \"class\",DecisionTreeClassifier(), \"accuracy\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9wGqoic8MTX"
   },
   "source": [
    "\n",
    "Se utilizó como métrica de Scoring el \"Accuracy\" teniendo en cuenta que es la medida más directa de la calidad de los clasificadores, en este caso Knn y Árboles de Decisión. Además, al ser una base de datos balanceada, es una buena idea usar esta métrica. El accuracy de este valor entre 0 y 1 se toma el mayor para seleccionar la mejor combinación de Parámetros o Pipeline. En este caso al ser tan sencilla la clasificación, la métrica es muy cercana a 1 con ambos clasificadores. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mNOckLDMM44"
   },
   "source": [
    "# **Exercise 2** (Filtering)\n",
    "\n",
    "1. Write a function analyze_filtering_benefits(X, y) in which you plug a feature se-lector (sklearn.feature_selection.SelectPercentile) together with (i) kNN and (i) DT into two pipelines. \n",
    "Compare the cross-validation performances of kNN/DT when filtering is used or notused on the given data, and report your findings. \n",
    "2. Run the filtering analysis on the madelon and amazon-commerce-reviewsdatasets. In-terpret your observations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jjiaLUpy8FM"
   },
   "source": [
    "2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfCY5_J2Xws8"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "#X = madelon_df.drop(\"Class\", axis=1)\n",
    "#y = madelon_df[\"Class\"]\n",
    "def analyze_filtering_benefits(X, y):\n",
    "    #scoring=accuracy_score\n",
    "    fea_selector=SelectPercentile(f_classif)\n",
    "    modelo_filtrado_dt=Pipeline([(\"filter\",fea_selector),(\"model\",DecisionTreeClassifier())])\n",
    "\n",
    "    modelo_filtrado_knn=Pipeline([(\"filter\",fea_selector),(\"model\",KNeighborsClassifier())])\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    #print(f\"Accuracy Score: {round(accuracy_score(y_train,predictions_dev)*100, 4)}%\")\n",
    "    print(f\"Accuracy Score for DT Full features: {round(np.mean(sklearn.model_selection.cross_validate(estimator=DecisionTreeClassifier(), X=X, y=y, cv=5, n_jobs=4, scoring = 'accuracy')['test_score'])*100, 2)}%\")\n",
    "    print(f\"Accuracy Score for DT filtered features: {round(np.mean(sklearn.model_selection.cross_validate(estimator=modelo_filtrado_dt, X=X, y=y, cv=5, n_jobs=4, scoring = 'accuracy')['test_score'])*100, 2)}%\")\n",
    "    \n",
    "    print(f\"Accuracy Score for KNN Full features: {round(np.mean(sklearn.model_selection.cross_validate(estimator=KNeighborsClassifier(), X=X, y=y, cv=5, n_jobs=4, scoring = 'accuracy')['test_score'])*100, 2)}%\")\n",
    "    print(f\"Accuracy Score for KNN filtered features: {round(np.mean(sklearn.model_selection.cross_validate(estimator=modelo_filtrado_knn, X=X, y=y, cv=5, n_jobs=4, scoring = 'accuracy')['test_score'])*100, 2)}%\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf_jQR7azoPM"
   },
   "outputs": [],
   "source": [
    "for i in list:\n",
    "  for j in lista:\n",
    "    modelo_filtrado_knn=Pipeline([(\"filter\",fea_selector),(\"model\",KNeighborsClassifier())])\n",
    "    print(f\"Accuracy Score for DT Full features: {round(np.mean(sklearn.model_selection.cross_validate(estimator=DecisionTreeClassifier(), X=X, y=y, cv=5, n_jobs=4, scoring = 'accuracy')['test_score'])*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-0K5Evmns_U"
   },
   "source": [
    "2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fccpIuO0nIud",
    "outputId": "94e9c78e-1575-471c-c9be-3b7d2201d4cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for DT Full features: 74.92%\n",
      "Accuracy Score for DT filtered features: 73.81%\n",
      "Accuracy Score for KNN Full features: 72.5%\n",
      "Accuracy Score for KNN filtered features: 83.62%\n"
     ]
    }
   ],
   "source": [
    "score=analyze_filtering_benefits (X_madelon,y_madelon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_oAc3zugPPR",
    "outputId": "a4d3b618-3be9-4681-a6c5-2ed9b6801de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for DT Full features: 40.2%\n",
      "Accuracy Score for DT filtered features: 41.73%\n",
      "Accuracy Score for KNN Full features: 25.0%\n",
      "Accuracy Score for KNN filtered features: 22.53%\n"
     ]
    }
   ],
   "source": [
    "score=analyze_filtering_benefits (X_amazon,y_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZglgnmQn91t"
   },
   "source": [
    "**Madelon**\n",
    "\n",
    "Una vez ejecutado el cross validation para la full data y la filtered data se evidencia que en el DT la pérdida en el accuracy es de ~1%. Sin embargo, al usar el 10% con mayor puntaje de anova para el KNN, hace que incremente el accuracy. \n",
    "\n",
    "**Amazon**\n",
    "\n",
    "Para el caso de Amazon, el resultado es contrario al de Madelon. \n",
    "Al comparar las ejecuciones para DT, usar el 10% de los atributos con mayor anova resulta en un ganancia marginal de Accuracy. Sin embargo, para el KNN resulta en una pérdida del 2.5% de la Full Data a la Filtered.\n",
    "\n",
    "Para ambos data sets, la pérdida marginal (y en los casos excepcionales la ganancia) son un argumento a favor de filtrar atributos para mejorar la rápidez del procesamiento sin perder accuracy del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sic2X3Ynw1O"
   },
   "source": [
    "#**Exercise 3**\n",
    "\n",
    "1. Write a function wrapping(learner, X, y, train_size, repeats, scoring, tries)that will randomly create tries subsets of attributes in X and evaluate the learner based on that sub-database using MCCV with the given train size and repeats, using the given scoring function. For each feature set, memorize the mean value of the scoring function. \n",
    "Return a pandas dataframe with m + 1 columns, where m is the number of attributes of X. There should be one line for every tried configuation, with the observed performance in the last cell. The other columns should be filled with 0/1 depending on whether anattribute was selected or not. \n",
    "\n",
    "2. Write a function analyze_wrapping_benefits(X, y)in which you create an analy-sis/test fold of X/y, and use (i) kNN and (i) DT with wrapping on the analysis set to find the best feature set using wrapping. Compare the performance of each of them. \n",
    "\n",
    "a) on the full analysis data and the projected analysis data. \n",
    "\n",
    "b) on the full test data and the projected test data.\n",
    "\n",
    "This should give 4 numbers in total per dataset (of course the results on the test datamust not be used for decision making), so this isnota function that you could use insideyour analysis (except that you separated test data before).\n",
    "\n",
    "3. Load the datasets madelon and amazon-commerce-reviewsfromopenml.org. You can use the API from openml.org.Run the wrapping analysis on both datasets. Interpret your observations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qaWZ61NzLC9"
   },
   "outputs": [],
   "source": [
    "def wrapping(learner, X, y, train_size, repeats, scoring, tries):\n",
    "    scores = []\n",
    "    scores_r = []\n",
    "    df=pd.DataFrame(data=list(X))\n",
    "    for i in range(1,repeats+1):\n",
    "        #iris_X=pd.DataFrame(data=X)\n",
    "        value=random.randint(1, X.shape[1])\n",
    "        sample= X.sample(n=value,axis='columns')\n",
    "        scores_r = []\n",
    "        for j in range(tries):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=train_size)\n",
    "            clf = learner().fit(X_train, y_train)\n",
    "            scores_r.append(scoring(y_train,clf.predict(X_train)))\n",
    "        score_f=sum(scores_r)/tries\n",
    "        scores.append(score_f)\n",
    "        df1=pd.DataFrame(data=list(sample))\n",
    "        df1[\"Rep\"+str(i)]=np.ones(sample.shape[1])\n",
    "        df=df.merge(df1,on=0,how='left').fillna(0)\n",
    "    df=df.T\n",
    "    df.columns=list(X)\n",
    "    df=df.drop(df.index[0])\n",
    "    df[\"scores\"]=scores\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFUqg64U7WjD"
   },
   "outputs": [],
   "source": [
    "df_knn=wrapping(KNeighborsClassifier,X_mushroom, y_mushroom, 0.75, 10, sklearn.metrics.matthews_corrcoef, 10)\n",
    "df_dt=wrapping(DecisionTreeClassifier,X_mushroom, y_mushroom, 0.75, 10, sklearn.metrics.matthews_corrcoef, 10)\n",
    "df=pd.concat([df_knn,df_dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6ZMImyM-Bgk",
    "outputId": "a270a41b-f1e0-4347-e025-6231c43bbbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Atributo Marca_A\n",
      "2     cap-shape_c_0.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "5     cap-shape_f_1.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "6     cap-shape_k_0.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "9     cap-shape_s_1.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "11    cap-shape_x_1.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "..                                    ...     ...\n",
      "212  population_s_0.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "213  population_s_1.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "224     habitat_m_0.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "231     habitat_w_1.0_1.0_1.0_1.0_1.0_1.0     1.0\n",
      "232                                scores     1.0\n",
      "\n",
      "[62 rows x 2 columns]\n",
      "62\n",
      "Accuracy Score Full Train Data: 100.0% with 232 attributes\n",
      "Accuracy Score Full Test Data: 100.0% with 232 attributes\n",
      "Accuracy Score Projected Train Data: 99.9824% with 62 attributes\n",
      "Accuracy Score Projected Test Data: 99.959% with 62 attributes\n"
     ]
    }
   ],
   "source": [
    "def analyze_wrapping_benefits(X, y):\n",
    "  df_knn=wrapping(KNeighborsClassifier,X, y, 0.75, 5, sklearn.metrics.accuracy_score, 3)\n",
    "  df_dt=wrapping(DecisionTreeClassifier,X, y, 0.75, 5, sklearn.metrics.accuracy_score, 3)\n",
    "  df=pd.concat([df_knn,df_dt])\n",
    "  df1=df\n",
    "  df1[\"scores1\"]=df1[\"scores\"].astype(str).str[:5]\n",
    "  df1[\"scores1\"]=df1[\"scores1\"].astype(float)\n",
    "  df1=df1.drop('scores',axis=1)\n",
    "  score_max=df1['scores1'].max()\n",
    "  df1=df1.loc[df1['scores1']==score_max]\n",
    "  df_max=df\n",
    "  df_max=df_max.merge(df1[[\"scores1\"]],left_index=True, right_index=True)\n",
    "  df_max=df.loc[df['scores1']==score_max]\n",
    "  df_max=df_max.drop(\"scores1\",axis=1)\n",
    "  df_max[\"Q_att\"]=df_max.sum(axis=1)\n",
    "  min_a=df_max[\"Q_att\"].min()\n",
    "  df_f=df_max[df_max[\"Q_att\"]==min_a]\n",
    "  mejor_rep=(df_f.T).reset_index()\n",
    "  mejor_rep.columns=[\"Atributo\",\"Marca_A\"]\n",
    "  mejor_rep=mejor_rep[mejor_rep[\"Marca_A\"]==1]\n",
    "  print(mejor_rep)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "  var=np.unique(mejor_rep[\"Atributo\"]).tolist()\n",
    "  print(len(var))\n",
    "  Xtr_pr=X_train[X_train.columns & var]\n",
    "  Xte_pr=X_test[X_test.columns & var]\n",
    "  clf_knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "  clf_knn_pr = KNeighborsClassifier().fit(Xtr_pr, y_train)\n",
    "  print(f\"Accuracy Score Full Train Data: {round(accuracy_score(y_train,clf_knn.predict(X_train))*100, 4)}% with {len(X.columns)} attributes\")\n",
    "  print(f\"Accuracy Score Full Test Data: {round(accuracy_score(y_test,clf_knn.predict(X_test))*100, 4)}% with {len(X.columns)} attributes\")\n",
    "  print(f\"Accuracy Score Projected Train Data: {round(accuracy_score(y_train,clf_knn_pr.predict(Xtr_pr))*100, 4)}% with {len(var)} attributes\")\n",
    "  print(f\"Accuracy Score Projected Test Data: {round(accuracy_score(y_test,clf_knn_pr.predict(Xte_pr))*100, 4)}% with {len(var)} attributes\")\n",
    "analyze_wrapping_benefits(X_mushroom, y_mushroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkFdeJayxp-E",
    "outputId": "8ba1274a-f74c-4a04-e93f-608babb8962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Atributo Marca_A\n",
      "1         V2     1.0\n",
      "2         V3     1.0\n",
      "3         V4     1.0\n",
      "4         V5     1.0\n",
      "5         V6     1.0\n",
      "..       ...     ...\n",
      "496     V497     1.0\n",
      "497     V498     1.0\n",
      "498     V499     1.0\n",
      "499     V500     1.0\n",
      "500   scores     1.0\n",
      "\n",
      "[338 rows x 2 columns]\n",
      "338\n",
      "Accuracy Score Full Train Data: 82.3529% with 500 attributes\n",
      "Accuracy Score Full Test Data: 72.3431% with 500 attributes\n",
      "Accuracy Score Projected Train Data: 81.5833% with 338 attributes\n",
      "Accuracy Score Projected Test Data: 67.6056% with 338 attributes\n"
     ]
    }
   ],
   "source": [
    "analyze_wrapping_benefits(X_madelon, y_madelon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3i9hPS_dDRlM",
    "outputId": "1db782be-13fe-4a0b-e0c1-411bd0885fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Atributo Marca_A\n",
      "2           V3     1.0\n",
      "4           V5     1.0\n",
      "6           V7     1.0\n",
      "9          V10     1.0\n",
      "10         V11     1.0\n",
      "...        ...     ...\n",
      "9984     V9985     1.0\n",
      "9989     V9990     1.0\n",
      "9990     V9991     1.0\n",
      "9991     V9992     1.0\n",
      "10000   scores     1.0\n",
      "\n",
      "[4127 rows x 2 columns]\n",
      "4127\n",
      "Accuracy Score Full Train Data: 54.9524% with 10000 attributes\n",
      "Accuracy Score Full Test Data: 23.3333% with 10000 attributes\n",
      "Accuracy Score Projected Train Data: 51.7143% with 4127 attributes\n",
      "Accuracy Score Projected Test Data: 22.4444% with 4127 attributes\n"
     ]
    }
   ],
   "source": [
    "analyze_wrapping_benefits(X_amazon, y_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDKHFHFtUPC2"
   },
   "source": [
    "**Madelon**\n",
    "\n",
    "Cuando se ejecuta la selección del set de atributos con mayor accuracy y buscando reducir la cantidad de atributos usados (para ayudar al procesamiento escalable), se nota una pérdida menor al 1% sobre el train data y menor al 4% sobre la test data. La decisión de incluir 338 (óptimo) o el total de 500 atributos, dependerá de la disposición que se tengan en recursos para ejecución y se quiera sumir su uso para obtener la ganancia marginal.\n",
    "\n",
    "**Amazon**\n",
    "\n",
    "Con la ejecución de Amazon el resultado es similar, usar un set de datos con menos del 42% del total de atribustos genera una pérdida (tanto en la train data como en la test data) de menos del 1%. En este caso la recomendación es conservar el modelo ejecutado con el subset óptimo siempre que el resultado es en la práctica el mismo. \n",
    "\n",
    "En ambas ejecuciones el resultado es similar. El trade off entre tener un modelo más sencillo y aumentar las metricas de performance, sugieren que es válido pensar en incluir menos atributos siempre que la pérdida en performance no es representativa en general.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c554a3a975d7cae87aff95066158061480c212e35c74fed8d47b5e8707d11cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
